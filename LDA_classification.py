import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from gensim import corpora, models
from gensim.models.coherencemodel import CoherenceModel
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from wordcloud import WordCloud
import io
import base64
from openai import OpenAI
from tqdm import tqdm
import re
import warnings
warnings.filterwarnings('ignore')

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (ì „ì—­)
import platform
import os

def setup_korean_font():
    """matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •"""
    system = platform.system()
    
    if system == "Windows":
        font_paths = [
            "C:/Windows/Fonts/malgun.ttf",
            "C:/Windows/Fonts/NanumGothic.ttf",
            "C:/Windows/Fonts/gulim.ttc",
        ]
    elif system == "Darwin":
        font_paths = [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/NanumGothic.ttf",
        ]
    else:
        font_paths = [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/TTF/NanumGothic.ttf",
        ]
    
    for font_path in font_paths:
        if os.path.exists(font_path):
            font_prop = fm.FontProperties(fname=font_path)
            plt.rcParams['font.family'] = font_prop.get_name()
            plt.rcParams['axes.unicode_minus'] = False
            return font_path
    
    return None

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
korean_font_path = setup_korean_font()

# ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì € import
try:
    from konlpy.tag import Okt
    korean_available = True
except ImportError:
    korean_available = False
    
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    english_available = True
except ImportError:
    english_available = False

# íŽ˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="LDA í† í”½ ëª¨ë¸ë§ ë„êµ¬", page_icon="ðŸ“Š", layout="wide")

# ì œëª©
st.title("ðŸ“Š LDA í† í”½ ëª¨ë¸ë§ ë„êµ¬")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ì„¤ì •")

# ê¸°ë³¸ ë¶ˆìš©ì–´ ì •ì˜
KOREAN_STOPWORDS = ['ë°', 'ì´', 'ê·¸', 'ë“±', 'ìˆ˜', 'ì˜', 'ì—', 'ë¥¼', 'ì„', 'ê°€', 'ìœ¼ë¡œ', 'ì—ì„œ', 
                   'í•˜ëŠ”', 'í•˜ì—¬', 'í•œë‹¤', 'ì†”ë£¨ì…˜', 'ëª©ì ', 'í¬í•¨', 'ìœ„í•´', 'ë‹¨ê³„', 'ë°©ë²•', 'ë¡œë¶€í„°',
                   'í†µí•´', 'ìœ„í•œ', 'ë”°ë¼', 'ëŒ€í•œ', 'ìžˆëŠ”', 'ìžˆë‹¤', 'ì—†ëŠ”', 'ì—†ë‹¤', 'ê°™ì€', 'ë‹¤ë¥¸',
                   'ë˜í•œ', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ëž˜ì„œ']

ENGLISH_STOPWORDS = ['and', 'or', 'but', 'the', 'a', 'an', 'is', 'are', 'was', 'were', 'been', 'be',
                    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
                    'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he',
                    'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his',
                    'her', 'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours', 'theirs', 'solution',
                    'purpose', 'method', 'step', 'include', 'for', 'from', 'through', 'with', 'by']

# ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì € í•¨ìˆ˜
def tokenize_korean(text, additional_stopwords=[]):
    if not korean_available:
        st.error("í•œêµ­ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ konlpyë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return []
    
    okt = Okt()
    stopwords = KOREAN_STOPWORDS + additional_stopwords
    tokens = okt.nouns(text)
    return [word for word in tokens if word not in stopwords and len(word) > 1]

def tokenize_english(text, additional_stopwords=[]):
    if not english_available:
        st.error("ì˜ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ nltkë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return []
    
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english')) | set(ENGLISH_STOPWORDS) | set(additional_stopwords)
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
    tokens = word_tokenize(text)
    
    # ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
    tokens = [lemmatizer.lemmatize(word) for word in tokens 
              if word not in stop_words and len(word) > 2]
    
    return tokens

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'df' not in st.session_state:
    st.session_state.df = None
if 'processed_texts' not in st.session_state:
    st.session_state.processed_texts = None
if 'dictionary' not in st.session_state:
    st.session_state.dictionary = None
if 'corpus' not in st.session_state:
    st.session_state.corpus = None
if 'lda_model' not in st.session_state:
    st.session_state.lda_model = None
if 'coherence_values' not in st.session_state:
    st.session_state.coherence_values = None
if 'model_list' not in st.session_state:
    st.session_state.model_list = None
if 'selected_column' not in st.session_state:
    st.session_state.selected_column = None
if 'language' not in st.session_state:
    st.session_state.language = None
if 'additional_stopwords_list' not in st.session_state:
    st.session_state.additional_stopwords_list = []
if 'topic_names' not in st.session_state:
    st.session_state.topic_names = None
if 'doc_topics' not in st.session_state:
    st.session_state.doc_topics = None

# 1. íŒŒì¼ ì—…ë¡œë“œ
st.header("1. íŒŒì¼ ì—…ë¡œë“œ")
uploaded_file = st.file_uploader("CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv', 'xlsx', 'xls'])

if uploaded_file is not None:
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file, encoding='utf-8')
        else:
            df = pd.read_excel(uploaded_file)
        
        st.session_state.df = df
        st.success(f"íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤! ({len(df)}í–‰, {len(df.columns)}ì—´)")
        
        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df.head())
        
    except Exception as e:
        st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")

# 2. ì»¬ëŸ¼ ì„ íƒ
if st.session_state.df is not None:
    st.header("2. ë¶„ì„í•  ì»¬ëŸ¼ ì„ íƒ")
    
    st.warning("âš ï¸ ì£¼ì˜ì‚¬í•­: ì„ íƒí•œ ì»¬ëŸ¼ ë‚´ì˜ ëª¨ë“  ë°ì´í„°ëŠ” ë™ì¼í•œ ì–¸ì–´ë¡œ ìž‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    text_columns = st.session_state.df.select_dtypes(include=['object']).columns.tolist()
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    default_index = 0
    if st.session_state.selected_column and st.session_state.selected_column in text_columns:
        default_index = text_columns.index(st.session_state.selected_column)
    
    selected_column = st.selectbox("LDA ë¶„ì„ì„ ìˆ˜í–‰í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:", 
                                  text_columns, 
                                  index=default_index,
                                  key="column_selector")
    
    # ì„¸ì…˜ì— ì €ìž¥
    st.session_state.selected_column = selected_column
    
    if selected_column:
        # ê²°ì¸¡ì¹˜ í™•ì¸ (ì›ë³¸ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  í‘œì‹œë§Œ)
        null_count = st.session_state.df[selected_column].isnull().sum()
        if null_count > 0:
            st.warning(f"ì„ íƒí•œ ì»¬ëŸ¼ì— {null_count}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ìžˆìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ ì‹œ ìžë™ìœ¼ë¡œ ì œê±°ë©ë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ì„ íƒí•œ ì»¬ëŸ¼ì˜ í…ìŠ¤íŠ¸ ì˜ˆì‹œ")
        preview_df = st.session_state.df.dropna(subset=[selected_column])
        st.text_area("í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°", value="\n".join(preview_df[selected_column].head(3).tolist()), height=150, label_visibility="collapsed")

# 3. ì–¸ì–´ ì„ íƒ ë° ì „ì²˜ë¦¬
if st.session_state.df is not None and st.session_state.selected_column:
    st.header("3. ì–¸ì–´ ì„ íƒ ë° ì „ì²˜ë¦¬")
    
    available_languages = []
    if korean_available and english_available:
        available_languages = ["í•œêµ­ì–´", "ì˜ì–´"]
    elif korean_available:
        available_languages = ["í•œêµ­ì–´"]
    elif english_available:
        available_languages = ["ì˜ì–´"]
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    default_lang_index = 0
    if st.session_state.language and st.session_state.language in available_languages:
        default_lang_index = available_languages.index(st.session_state.language)
    
    language = st.selectbox("í…ìŠ¤íŠ¸ ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", 
                           available_languages,
                           index=default_lang_index,
                           key="language_selector")
    
    # ì„¸ì…˜ì— ì €ìž¥
    st.session_state.language = language
    
    if language:
        # ì¶”ê°€ ë¶ˆìš©ì–´ ìž…ë ¥
        st.subheader("ì¶”ê°€ ë¶ˆìš©ì–´ ì„¤ì •")
        
        # ê¸°ë³¸ ë¶ˆìš©ì–´ í‘œì‹œ
        if language == "í•œêµ­ì–´":
            st.text("ê¸°ë³¸ ë¶ˆìš©ì–´: " + ", ".join(KOREAN_STOPWORDS[:10]) + "...")
        else:
            st.text("ê¸°ë³¸ ë¶ˆìš©ì–´: " + ", ".join(ENGLISH_STOPWORDS[:10]) + "...")
        
        # ê¸°ì¡´ ì¶”ê°€ ë¶ˆìš©ì–´ê°€ ìžˆë‹¤ë©´ í‘œì‹œ
        existing_stopwords = ", ".join(st.session_state.additional_stopwords_list) if st.session_state.additional_stopwords_list else ""
        
        additional_stopwords = st.text_input("ì¶”ê°€ ë¶ˆìš©ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„):", 
                                           value=existing_stopwords,
                                           key="additional_stopwords")
        additional_stopwords_list = [word.strip() for word in additional_stopwords.split(",") if word.strip()]
        
        # ì„¸ì…˜ì— ì €ìž¥
        st.session_state.additional_stopwords_list = additional_stopwords_list
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        if st.button("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤í–‰", key="preprocess_btn"):
            # ê²°ì¸¡ì¹˜ ì œê±°ëœ ë°ì´í„° ì‚¬ìš©
            clean_df = st.session_state.df.dropna(subset=[st.session_state.selected_column])
            texts = clean_df[st.session_state.selected_column].astype(str).tolist()
            
            with st.spinner("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘..."):
                if language == "í•œêµ­ì–´":
                    processed_texts = [tokenize_korean(text, additional_stopwords_list) for text in texts]
                else:
                    processed_texts = [tokenize_english(text, additional_stopwords_list) for text in texts]
                
                # ë¹ˆ ë¬¸ì„œ ì œê±°
                processed_texts = [text for text in processed_texts if text]
                
                if len(processed_texts) == 0:
                    st.error("ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶ˆìš©ì–´ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    st.session_state.processed_texts = processed_texts
                    
                    # ë”•ì…”ë„ˆë¦¬ ë° ì½”í¼ìŠ¤ ìƒì„±
                    dictionary = corpora.Dictionary(processed_texts)
                    corpus = [dictionary.doc2bow(text) for text in processed_texts]
                    
                    st.session_state.dictionary = dictionary
                    st.session_state.corpus = corpus
                    
                    st.success(f"ì „ì²˜ë¦¬ ì™„ë£Œ! {len(processed_texts)}ê°œ ë¬¸ì„œ, ë‹¨ì–´ ìˆ˜: {len(dictionary)}")
                    
                    # ì „ì²˜ë¦¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                    st.subheader("ì „ì²˜ë¦¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
                    for i, tokens in enumerate(processed_texts[:3]):
                        st.text(f"ë¬¸ì„œ {i+1}: " + " ".join(tokens[:20]) + "...")
        
        # ì „ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
        if st.session_state.processed_texts is not None:
            st.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œë¨: {len(st.session_state.processed_texts)}ê°œ ë¬¸ì„œ, {len(st.session_state.dictionary) if st.session_state.dictionary else 0}ê°œ ë‹¨ì–´")

# 4. ìµœì  í† í”½ ìˆ˜ íƒìƒ‰
if st.session_state.processed_texts is not None:
    st.header("4. ìµœì  í† í”½ ìˆ˜ íƒìƒ‰")
    
    col1, col2 = st.columns(2)
    with col1:
        start_topics = st.number_input("ì‹œìž‘ í† í”½ ìˆ˜", min_value=2, max_value=20, value=2, key="start_topics")
    with col2:
        end_topics = st.number_input("ì¢…ë£Œ í† í”½ ìˆ˜", min_value=3, max_value=30, value=10, key="end_topics")
    
    if st.button("í† í”½ ìˆ˜ íƒìƒ‰ ì‹¤í–‰", key="coherence_search_btn"):
        if start_topics >= end_topics:
            st.error("ì‹œìž‘ í† í”½ ìˆ˜ëŠ” ì¢…ë£Œ í† í”½ ìˆ˜ë³´ë‹¤ ìž‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("ìµœì  í† í”½ ìˆ˜ë¥¼ íƒìƒ‰ ì¤‘ìž…ë‹ˆë‹¤..."):
                coherence_values = []
                model_list = []
                
                progress_bar = st.progress(0)
                for i, num_topics in enumerate(range(start_topics, end_topics + 1)):
                    model = models.LdaModel(
                        corpus=st.session_state.corpus,
                        id2word=st.session_state.dictionary,
                        num_topics=num_topics,
                        random_state=42,
                        passes=10,
                        alpha='auto',
                        eta='auto'
                    )
                    model_list.append(model)
                    
                    coherence_model = CoherenceModel(
                        model=model,
                        texts=st.session_state.processed_texts,
                        dictionary=st.session_state.dictionary,
                        coherence='c_v'
                    )
                    coherence_values.append(coherence_model.get_coherence())
                    
                    progress_bar.progress((i + 1) / (end_topics - start_topics + 1))
                
                st.session_state.coherence_values = coherence_values
                st.session_state.model_list = model_list
                st.session_state.coherence_start = start_topics
                st.session_state.coherence_end = end_topics
                
                # ê·¸ëž˜í”„ ê·¸ë¦¬ê¸°
                fig, ax = plt.subplots(figsize=(10, 6))
                x = range(start_topics, end_topics + 1)
                ax.plot(x, coherence_values, 'b-', marker='o')
                ax.set_xlabel('í† í”½ ìˆ˜')
                ax.set_ylabel('Coherence Score')
                ax.set_title('ìµœì  í† í”½ ìˆ˜ íƒìƒ‰')
                ax.grid(True)
                
                # ìµœê³  ì ìˆ˜ í‘œì‹œ
                max_idx = coherence_values.index(max(coherence_values))
                optimal_topics = start_topics + max_idx
                ax.annotate(f'ìµœì : {optimal_topics}', 
                           xy=(optimal_topics, coherence_values[max_idx]),
                           xytext=(optimal_topics + 1, coherence_values[max_idx] + 0.01),
                           arrowprops=dict(arrowstyle='->', color='red'))
                
                st.pyplot(fig)
                
                st.success(f"íƒìƒ‰ ì™„ë£Œ! ê¶Œìž¥ í† í”½ ìˆ˜: {optimal_topics} (Coherence: {coherence_values[max_idx]:.4f})")
    
    # ê¸°ì¡´ ê²°ê³¼ê°€ ìžˆë‹¤ë©´ í‘œì‹œ
    elif st.session_state.coherence_values is not None:
        st.info("âœ… í† í”½ ìˆ˜ íƒìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ëž˜ì—ì„œ ìµœì¢… í† í”½ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        
        # ê¸°ì¡´ ê·¸ëž˜í”„ ë‹¤ì‹œ í‘œì‹œ
        fig, ax = plt.subplots(figsize=(10, 6))
        x = range(st.session_state.coherence_start, st.session_state.coherence_end + 1)
        ax.plot(x, st.session_state.coherence_values, 'b-', marker='o')
        ax.set_xlabel('í† í”½ ìˆ˜')
        ax.set_ylabel('Coherence Score')
        ax.set_title('ìµœì  í† í”½ ìˆ˜ íƒìƒ‰')
        ax.grid(True)
        
        # ìµœê³  ì ìˆ˜ í‘œì‹œ
        max_idx = st.session_state.coherence_values.index(max(st.session_state.coherence_values))
        optimal_topics = st.session_state.coherence_start + max_idx
        ax.annotate(f'ìµœì : {optimal_topics}', 
                   xy=(optimal_topics, st.session_state.coherence_values[max_idx]),
                   xytext=(optimal_topics + 1, st.session_state.coherence_values[max_idx] + 0.01),
                   arrowprops=dict(arrowstyle='->', color='red'))
        
        st.pyplot(fig)

# 5. í† í”½ ìˆ˜ ì„ íƒ ë° LDA ëª¨ë¸ ìƒì„±
if st.session_state.coherence_values is not None or st.session_state.processed_texts is not None:
    st.header("5. ìµœì¢… í† í”½ ìˆ˜ ì„ íƒ")
    
    optimal_num = st.number_input("ì‚¬ìš©í•  í† í”½ ìˆ˜ë¥¼ ìž…ë ¥í•˜ì„¸ìš”:", min_value=2, max_value=50, value=3, key="final_topic_num")
    
    if st.button("LDA ëª¨ë¸ ìƒì„±", key="lda_create_btn"):
        with st.spinner("LDA ëª¨ë¸ì„ ìƒì„± ì¤‘ìž…ë‹ˆë‹¤..."):
            lda_model = models.LdaModel(
                corpus=st.session_state.corpus,
                id2word=st.session_state.dictionary,
                num_topics=optimal_num,
                random_state=42,
                passes=20,
                alpha='auto',
                eta='auto'
            )
            
            st.session_state.lda_model = lda_model
            st.success("LDA ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ê¸°ì¡´ ëª¨ë¸ ìƒíƒœ í‘œì‹œ
    if st.session_state.lda_model is not None:
        st.info(f"âœ… LDA ëª¨ë¸ ìƒì„± ì™„ë£Œ: {st.session_state.lda_model.num_topics}ê°œ í† í”½")

# 6. pyLDAvis ì‹œê°í™”
if st.session_state.lda_model is not None:
    st.header("6. í† í”½ ì‹œê°í™” (pyLDAvis)")
    
    st.info("ðŸ’¡ ì›ë“¤ì´ ì„œë¡œ ê²¹ì¹˜ì§€ ì•Šì„ ë•Œ í† í”½ì´ ì ì ˆí•˜ê²Œ ë¶„ë¦¬ëœ ê²ƒìž…ë‹ˆë‹¤. ì›ì´ ë§Žì´ ê²¹ì¹œë‹¤ë©´ í† í”½ ìˆ˜ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
    
    if st.button("í† í”½ ì‹œê°í™” ìƒì„±"):
        with st.spinner("ì‹œê°í™”ë¥¼ ìƒì„± ì¤‘ìž…ë‹ˆë‹¤..."):
            try:
                vis = gensimvis.prepare(st.session_state.lda_model, st.session_state.corpus, st.session_state.dictionary)
                html_string = pyLDAvis.prepared_data_to_html(vis)
                st.components.v1.html(html_string, height=800)
            except Exception as e:
                st.error(f"ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {str(e)}")

# 7. í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¡°íšŒ
if st.session_state.lda_model is not None:
    st.header("7. í† í”½ë³„ ìƒìœ„ ë‹¨ì–´")
    
    num_words = st.number_input("í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ê°œìˆ˜", min_value=10, max_value=500, value=100, key="num_words_display")
    st.info("ðŸ’¡ í† í”½ëª…ì„ ì •í•˜ê¸° ìœ„í•´ì„œëŠ” 100~150ê°œì˜ ë‹¨ì–´ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì ì ˆí•©ë‹ˆë‹¤.")
    
    # ìžë™ìœ¼ë¡œ í† í”½ë³„ ë‹¨ì–´ í‘œì‹œ (ë²„íŠ¼ ì—†ì´)
    st.subheader("í† í”½ë³„ ì£¼ìš” ë‹¨ì–´")
    for i, topic in st.session_state.lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False):
        with st.expander(f"í† í”½ {i} - ìƒìœ„ {num_words}ê°œ ë‹¨ì–´"):
            words = [word for word, prob in topic]
            st.write(", ".join(words))

# 8. GPT APIë¥¼ í†µí•œ í† í”½ëª… ìƒì„±
if st.session_state.lda_model is not None:
    st.header("8. AI ê¸°ë°˜ í† í”½ëª… ìƒì„±")
    
    openai_api_key = st.text_input("OpenAI API í‚¤ë¥¼ ìž…ë ¥í•˜ì„¸ìš”:", type="password", key="openai_key")
    domain_context = st.text_area("ì „ì²´ ë°ì´í„°ì˜ ë„ë©”ì¸ ì„¤ëª… (ì„ íƒì‚¬í•­):", 
                                  placeholder="ì˜ˆ: ì´ ë°ì´í„°ëŠ” AI ê´€ë ¨ íŠ¹í—ˆ ë¬¸ì„œë“¤ìž…ë‹ˆë‹¤.",
                                  key="domain_context")
    
    # GPTì— ì „ë‹¬í•  ë‹¨ì–´ ê°œìˆ˜ ì„¤ì •
    gpt_num_words = st.number_input("GPT í† í”½ëª… ìƒì„±ì— ì‚¬ìš©í•  ë‹¨ì–´ ê°œìˆ˜", 
                                    min_value=20, max_value=200, value=50, 
                                    key="gpt_num_words",
                                    help="í† í”½ëª… ìƒì„±ì„ ìœ„í•´ GPTì—ê²Œ ì „ë‹¬í•  ê° í† í”½ë‹¹ ë‹¨ì–´ ê°œìˆ˜")
    
    if st.button("í† í”½ëª… ìƒì„±", key="topic_naming_btn") and openai_api_key:
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìƒˆ ë²„ì „ ë°©ì‹)
            client = OpenAI(api_key=openai_api_key)
            
            # ê° í† í”½ì˜ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ (ì‚¬ìš©ìž ì„¤ì • ê°œìˆ˜ ì‚¬ìš©)
            topics_words = []
            for i, topic in st.session_state.lda_model.show_topics(num_topics=-1, num_words=gpt_num_words, formatted=False):
                words = [word for word, prob in topic]
                topics_words.append(f"í† í”½ {i}: {', '.join(words)}")
            
            # GPT í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""
ë‹¤ìŒì€ LDA í† í”½ ëª¨ë¸ë§ ê²°ê³¼ìž…ë‹ˆë‹¤. ê° í† í”½ì— ëŒ€í•´ ì ì ˆí•œ í† í”½ëª…ì„ í•œêµ­ì–´ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”.

{domain_context}

í† í”½ë³„ ì£¼ìš” ë‹¨ì–´:
{chr(10).join(topics_words)}

ìš”ì²­ì‚¬í•­:
1. ê° í† í”½ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì˜ë¯¸ìžˆëŠ” í† í”½ëª…ì„ ì œì•ˆ
2. í† í”½ ê°„ ë‚´ìš© ì¤‘ë³µ ì—¬ë¶€ ë¶„ì„
3. í† í”½ ê°œìˆ˜ ì¡°ì • í•„ìš”ì„±ì— ëŒ€í•œ ì˜ê²¬ ì œì‹œ

í˜•ì‹:
í† í”½ 0: [í† í”½ëª…]
í† í”½ 1: [í† í”½ëª…]
...

ë¶„ì„ ì˜ê²¬:
[í† í”½ ì¤‘ë³µì„± ë° ê°œìˆ˜ ì¡°ì • í•„ìš”ì„±ì— ëŒ€í•œ ì˜ê²¬]
"""
            
            with st.spinner("AIê°€ í† í”½ëª…ì„ ìƒì„± ì¤‘ìž…ë‹ˆë‹¤..."):
                # ìƒˆ OpenAI API ë°©ì‹
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=1000,
                    temperature=0.3
                )
                
                ai_response = response.choices[0].message.content
                st.success("AI í† í”½ëª… ìƒì„± ì™„ë£Œ!")
                st.write(ai_response)
                
                # ì„¸ì…˜ì— ì €ìž¥
                st.session_state.topic_names = ai_response
                
        except Exception as e:
            st.error(f"AI í† í”½ëª… ìƒì„± ì˜¤ë¥˜: {str(e)}")
            st.info("ðŸ’¡ í•´ê²°ë°©ë²•: OpenAI API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ê±°ë‚˜, êµ¬ë²„ì „ ì‚¬ìš©ì„ ì›í•œë‹¤ë©´ `pip install openai==0.28.1`ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # ê¸°ì¡´ í† í”½ëª…ì´ ìžˆë‹¤ë©´ í‘œì‹œ
    if st.session_state.topic_names is not None:
        st.subheader("ìƒì„±ëœ í† í”½ëª…")
        st.write(st.session_state.topic_names)

# 9. ê° í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ
if st.session_state.lda_model is not None:
    st.header("9. í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ")
    
    num_docs = st.number_input("í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ ê°œìˆ˜", min_value=1, max_value=5, value=3, key="num_docs_display")
    
    # ë¬¸ì„œ í† í”½ í• ë‹¹ì´ ì—†ë‹¤ë©´ ê³„ì‚°
    if st.session_state.doc_topics is None:
        # ê° ë¬¸ì„œì˜ ì£¼ìš” í† í”½ ê²°ì •
        doc_topics = []
        for doc_bow in st.session_state.corpus:
            topic_probs = st.session_state.lda_model.get_document_topics(doc_bow)
            if topic_probs:
                top_topic = max(topic_probs, key=lambda x: x[1])[0]
                doc_topics.append(top_topic)
            else:
                doc_topics.append(-1)
        st.session_state.doc_topics = doc_topics
    
    # ìžë™ìœ¼ë¡œ ëŒ€í‘œ ë¬¸ì„œ í‘œì‹œ
    st.subheader("í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ")
    
    # ê²°ì¸¡ì¹˜ê°€ ì œê±°ëœ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
    clean_df = st.session_state.df.dropna(subset=[st.session_state.selected_column]).reset_index(drop=True)
    clean_df['LDA_í† í”½'] = st.session_state.doc_topics
    
    for topic_num in range(st.session_state.lda_model.num_topics):
        with st.expander(f"í† í”½ {topic_num} ëŒ€í‘œ ë¬¸ì„œ ({num_docs}ê°œ)"):
            topic_docs = clean_df[clean_df['LDA_í† í”½'] == topic_num]
            
            if len(topic_docs) > 0:
                sample_docs = topic_docs.head(num_docs)
                for idx, (_, row) in enumerate(sample_docs.iterrows()):
                    st.write(f"**ë¬¸ì„œ {idx+1}:**")
                    st.write(row[st.session_state.selected_column])
                    st.write("---")
            else:
                st.write("ì´ í† í”½ì— í• ë‹¹ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

# 10. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
if st.session_state.lda_model is not None:
    st.header("10. í† í”½ë³„ ì›Œë“œí´ë¼ìš°ë“œ")
    
    if st.button("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="wordcloud_btn"):
        # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì„¤ì •
        import platform
        import os
        from matplotlib import font_manager
        
        def get_korean_font_path():
            system = platform.system()
            
            # Windows
            if system == "Windows":
                font_paths = [
                    "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
                    "C:/Windows/Fonts/NanumGothic.ttf",  # ë‚˜ëˆ”ê³ ë”•
                    "C:/Windows/Fonts/gulim.ttc",  # êµ´ë¦¼
                ]
            # macOS
            elif system == "Darwin":
                font_paths = [
                    "/System/Library/Fonts/AppleSDGothicNeo.ttc",  # ì• í”Œ SD ê³ ë”• Neo
                    "/Library/Fonts/NanumGothic.ttf",  # ë‚˜ëˆ”ê³ ë”•
                    "/System/Library/Fonts/Helvetica.ttc",
                ]
            # Linux
            else:
                font_paths = [
                    "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # ë‚˜ëˆ”ê³ ë”•
                    "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
                    "/usr/share/fonts/TTF/NanumGothic.ttf",
                ]
            
            # ì¡´ìž¬í•˜ëŠ” ì²« ë²ˆì§¸ í°íŠ¸ ë°˜í™˜
            for font_path in font_paths:
                if os.path.exists(font_path):
                    return font_path
            
            return None
        
        korean_font = get_korean_font_path()
        
        if korean_font is None:
            st.warning("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¬¸ìœ¼ë¡œë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
            st.info("""
            ðŸ’¡ í•œê¸€ í°íŠ¸ ì„¤ì¹˜ ë°©ë²•:
            - **Windows**: ê¸°ë³¸ì ìœ¼ë¡œ ë§‘ì€ê³ ë”•ì´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤
            - **macOS**: ì‹œìŠ¤í…œ í°íŠ¸ ì‚¬ìš©
            - **Linux**: `sudo apt-get install fonts-nanum` ì‹¤í–‰
            """)
        
        # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
        if korean_font:
            font_prop = font_manager.FontProperties(fname=korean_font)
            plt.rcParams['font.family'] = font_prop.get_name()
            plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
        
        for i, topic in st.session_state.lda_model.show_topics(num_words=100, formatted=False):
            word_freq = {word: prob for word, prob in topic}
            
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            wordcloud = WordCloud(
                font_path=korean_font,  # í•œê¸€ í°íŠ¸ ì§€ì •
                background_color='white',
                width=800,
                height=400,
                max_words=100,
                colormap='viridis',  # ìƒ‰ìƒ í…Œë§ˆ
                relative_scaling=0.5,
                random_state=42
            ).generate_from_frequencies(word_freq)
            
            # í”Œë¡¯ ìƒì„±
            fig, ax = plt.subplots(figsize=(12, 6))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')
            
            # í•œê¸€ ì œëª© ì„¤ì •
            if korean_font:
                ax.set_title(f'í† í”½ {i} ì›Œë“œí´ë¼ìš°ë“œ', fontproperties=font_prop, fontsize=16, pad=20)
            else:
                ax.set_title(f'Topic {i} WordCloud', fontsize=16, pad=20)
            
            st.pyplot(fig)
            plt.close()  # ë©”ëª¨ë¦¬ í•´ì œ
            
        # matplotlib í°íŠ¸ ì„¤ì • ë³µì›
        plt.rcParams.update(plt.rcParamsDefault)

# 11. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
if st.session_state.lda_model is not None:
    st.header("11. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    
    if st.button("ê²°ê³¼ íŒŒì¼ ìƒì„±", key="download_btn"):
        # ë¬¸ì„œ í† í”½ í• ë‹¹ì´ ì—†ë‹¤ë©´ ê³„ì‚°
        if st.session_state.doc_topics is None:
            doc_topics = []
            for doc_bow in st.session_state.corpus:
                topic_probs = st.session_state.lda_model.get_document_topics(doc_bow)
                if topic_probs:
                    top_topic = max(topic_probs, key=lambda x: x[1])[0]
                    doc_topics.append(top_topic)
                else:
                    doc_topics.append(-1)
            st.session_state.doc_topics = doc_topics
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆìž„ ìƒì„± (ê²°ì¸¡ì¹˜ ì œê±°ëœ ë°ì´í„° ì‚¬ìš©)
        result_df = st.session_state.df.dropna(subset=[st.session_state.selected_column]).reset_index(drop=True)
        result_df['LDA_í† í”½'] = st.session_state.doc_topics
        
        # GPT í† í”½ëª…ì´ ìžˆë‹¤ë©´ ì¶”ê°€
        if st.session_state.topic_names is not None:
            # GPT ì‘ë‹µì—ì„œ í† í”½ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
            topic_name_map = {}
            lines = st.session_state.topic_names.split('\n')
            for line in lines:
                if 'í† í”½' in line and ':' in line:
                    try:
                        topic_num = int(line.split('í† í”½')[1].split(':')[0].strip())
                        topic_name = line.split(':')[1].strip()
                        topic_name_map[topic_num] = topic_name
                    except:
                        continue
            
            result_df['í† í”½ëª…'] = result_df['LDA_í† í”½'].map(topic_name_map)
        
        # CSV íŒŒì¼ë¡œ ë³€í™˜
        csv_buffer = io.StringIO()
        result_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ðŸ“¥ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name="LDA_ë¶„ì„ê²°ê³¼.csv",
            mime="text/csv",
            key="download_csv_btn"
        )
        
        st.success("ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ê²°ê³¼ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(result_df.head())

# ì‚¬ì´ë“œë°” ì •ë³´
st.sidebar.markdown("---")
st.sidebar.subheader("ðŸ“– ì‚¬ìš© ê°€ì´ë“œ")
st.sidebar.markdown("""
1. CSV/Excel íŒŒì¼ ì—…ë¡œë“œ
2. ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ
3. ì–¸ì–´ ì„ íƒ ë° ë¶ˆìš©ì–´ ì„¤ì •
4. ìµœì  í† í”½ ìˆ˜ íƒìƒ‰
5. LDA ëª¨ë¸ ìƒì„±
6. ê²°ê³¼ ë¶„ì„ ë° ë‹¤ìš´ë¡œë“œ
""")

st.sidebar.markdown("---")
st.sidebar.subheader("âš™ï¸ ìš”êµ¬ì‚¬í•­")
st.sidebar.markdown("""
**Python íŒ¨í‚¤ì§€:**
- streamlit
- pandas
- gensim
- pyLDAvis
- wordcloud
- openai>=1.0.0
- konlpy (í•œêµ­ì–´)
- nltk (ì˜ì–´)
- matplotlib

**OpenAI API ì°¸ê³ :**
- ìƒˆ ë²„ì „: openai>=1.0.0
- êµ¬ ë²„ì „: openai==0.28.1
""")